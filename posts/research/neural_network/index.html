<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>全连接神经网络 - SivanLaai's Blog</title><meta name=author content="SivanLaai"><meta name=author-link content="https://www.laais.cn"><meta name=description content="再学完斯坦福大学的计算机视觉的课后，总结一下自己对全连接神经网络的理解，方便在以后自己可以查阅和复习。首先简单的复习一下神经网络的概念，神经"><meta name=keywords content="神经网络"><meta itemprop=name content="全连接神经网络"><meta itemprop=description content="再学完斯坦福大学的计算机视觉的课后，总结一下自己对全连接神经网络的理解，方便在以后自己可以查阅和复习。首先简单的复习一下神经网络的概念，神经"><meta itemprop=datePublished content="2018-04-22T20:14:00+00:00"><meta itemprop=dateModified content="2023-03-28T13:49:36+08:00"><meta itemprop=wordCount content="1511"><meta itemprop=image content="https://www.laais.cn/avatar.png"><meta itemprop=keywords content="神经网络,"><meta property="og:title" content="全连接神经网络"><meta property="og:description" content="再学完斯坦福大学的计算机视觉的课后，总结一下自己对全连接神经网络的理解，方便在以后自己可以查阅和复习。首先简单的复习一下神经网络的概念，神经"><meta property="og:type" content="article"><meta property="og:url" content="https://www.laais.cn/posts/research/neural_network/"><meta property="og:image" content="https://www.laais.cn/avatar.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-04-22T20:14:00+00:00"><meta property="article:modified_time" content="2023-03-28T13:49:36+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://www.laais.cn/avatar.png"><meta name=twitter:title content="全连接神经网络"><meta name=twitter:description content="再学完斯坦福大学的计算机视觉的课后，总结一下自己对全连接神经网络的理解，方便在以后自己可以查阅和复习。首先简单的复习一下神经网络的概念，神经"><meta name=application-name content="Sivanlaai's blog"><meta name=apple-mobile-web-app-title content="Sivanlaai's blog"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://www.laais.cn/posts/research/neural_network/><link rel=prev href=https://www.laais.cn/posts/research/softmax/><link rel=next href=https://www.laais.cn/posts/research/bacth_norm/><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"全连接神经网络","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/www.laais.cn\/posts\/research\/neural_network\/"},"image":["https:\/\/www.laais.cn\/apple-touch-icon.png"],"genre":"posts","keywords":"神经网络","wordcount":1511,"url":"https:\/\/www.laais.cn\/posts\/research\/neural_network\/","datePublished":"2018-04-22T20:14:00+00:00","dateModified":"2023-03-28T13:49:36+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"SivanLaai","logo":"https:\/\/www.laais.cn\/avatar.png"},"author":{"@type":"Person","name":"SivanLaai"},"description":""}</script></head><body data-header-desktop=sticky data-header-mobile=auto><script>(window.localStorage?.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("data-theme","dark")</script><div class=wrapper data-page-style=custom><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="SivanLaai's Blog"><img loading=lazy src=/avatar.png srcset="/avatar.png, /avatar.png 1.5x, /avatar.png 2x" sizes=auto data-title="SivanLaai's Blog" data-alt="SivanLaai's Blog" class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>SivanLaai's blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/ccf title=CCF推荐><i class="fa-solid fa-book fa-fw fa-sm" aria-hidden=true></i> CCF</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item language"><span role=button aria-label title>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span><ul class=sub-menu><li class=menu-item></li></ul></li><li class="menu-item search" id=search-desktop><input type=text placeholder id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="SivanLaai's Blog"><img loading=lazy src=/avatar.png srcset="/avatar.png, /avatar.png 1.5x, /avatar.png 2x" sizes=auto data-title=/avatar.png data-alt=/avatar.png class=logo style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'><span class=header-title-text>SivanLaai's blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title><i class="fa-solid fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile></a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 所有文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/ccf title=CCF推荐><i class="fa-solid fa-book fa-fw fa-sm" aria-hidden=true></i> CCF</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item text-center"><a class=menu-link href=https://github.com/SivanLaai/blog title=GitHub rel="noopener noreferrer" target=_blank><i class='fa-brands fa-github fa-fw' aria-hidden=true></i></a></li><li class="menu-item theme-switch" title><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li><li class="menu-item language"><span role=button aria-label title>简体中文<i class="dropdown-icon fa-solid fa-chevron-down" aria-hidden=true></i></span>
<select class=language-select onchange="location=this.value"><option disabled></option></select></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?aa4e404535364259553aa4bd8f89bb3a",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><main class=container><aside class=toc id=toc-auto><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside><aside class=aside-custom></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>全连接神经网络</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://www.laais.cn title=作者 target=_blank rel="external nofollow noopener noreferrer author" class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
SivanLaai</a></span>
<span class=post-category>收录于 <a href=/categories/%E7%AC%94%E8%AE%B0/><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 笔记</a></span></div><div class=post-meta-line><span title="发布于 2018-04-22 20:14:00"><i class="fa-regular fa-calendar-alt fa-fw me-1" aria-hidden=true></i><time datetime=2018-04-22>2018-04-22</time></span>&nbsp;<span title="更新于 2023-03-28 13:49:36"><i class="fa-regular fa-edit fa-fw me-1" aria-hidden=true></i><time datetime=2023-03-28>2023-03-28</time></span>&nbsp;<span><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 1511 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 4 分钟</span>&nbsp;<span class=comment-visitors data-flag-title=全连接神经网络>
<i class="fa-regular fa-eye fa-fw me-1" aria-hidden=true></i><span data-path=/posts/research/neural_network/ class=waline-pageview-count>-</span>&nbsp;次阅读
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1神经网络中的基本结构>1.神经网络中的基本结构</a></li><li><a href=#2前向传播>2.前向传播</a></li><li><a href=#3反向传播>3.反向传播</a></li><li><a href=#4权重初始化>4.权重初始化</a></li><li><a href=#5神经网络中的梯度下降>5.神经网络中的梯度下降</a></li></ul></nav></div></div><div class=content id=content><div class="details admonition warning open"><div class="details-summary admonition-title"><i class="icon fa-solid fa-exclamation-triangle fa-fw" aria-hidden=true></i>警告<i class="details-icon fa-solid fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content>本文最后更新于 2023-03-28，文中内容可能已过时。</div></div></div><p>   再学完斯坦福大学的计算机视觉的课后，总结一下自己对全连接神经网络的理解，方便在以后自己可以查阅和复习。首先简单的复习一下神经网络的概念，神经网络有输入层、隐藏层和输出层三种层，其中隐藏层可能会有多层，一个神经网络有多少层要看有多少个隐藏层加上输出层就为该神经网络的层数。神经网络的来源源于生物体的大脑的神经元的触发机制，但是我们要区分神经网络和真实生物体的差别。神经网络不是生物体神经元的真实映射。本篇文章以cs231n中的作业二中的神经网络作为背景进行讲述。</p><h2 id=1神经网络中的基本结构>1.神经网络中的基本结构</h2><p><img loading=lazy src=/images/neuralnetwork.png srcset="/images/neuralnetwork.png, /images/neuralnetwork.png 1.5x, /images/neuralnetwork.png 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
   如上图所示，神经网络有输入层、隐藏层和输出层组成，这个神经网络一共有两层，一个隐藏层和一个输出层，输入层不算层数。输入层有输入维度为3，第一层有四个神经元，输出层有两个神经元。在某些结构中，我们的神经网络结构会更加，隐藏层可能会不只一个，而且每一层的神经元个数也会不唯一。其中每一个神经元有一个输入和一个输出，如下为一个神经元的内部详解：
<img loading=lazy src=/images/activate.png srcset="/images/activate.png, /images/activate.png 1.5x, /images/activate.png 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
一个神经元其实有两个处理，首先是对前面的输入做一个线性求和$$Z = \sum_{i=1}^{N} wi \cdot xi +b$$
然后在有一个激活函数f在对z做处理得到这个神经元的输出
$$f(\sum_{i=1}^{N} wi \cdot xi +b)$$
讲完了基本的神经网络结构后，我们现在以cs231n中作业2中的全连接神经网络架构做一个讲述，他的架构为{affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax也就是说前面的L-1层的每一层，先做一个affine，然后batch norm 在接着激活函数用relu处理一下输出，最后做一个dropout，到最后一层就在一个affine后进入一个softmax层得到神经网络的最终输出。</p><h2 id=2前向传播>2.前向传播</h2><p>   前向传播是从输入层开始把每一层的输出递交给下一层直至最后一层将结果输出的过程，在cs231n中前面的L-1层的前向过程如下：
affine层对输入做一个线性组合输出affOut：
$$affOut = \sum_{i=1}^{N} wi \cdot xi +b$$
batch norm层：
<img loading=lazy src=/images/batchnorm.png srcset="/images/batchnorm.png, /images/batchnorm.png 1.5x, /images/batchnorm.png 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
得到输出batchOUt
relu层：
<img loading=lazy src=/images/relu.png srcset="/images/relu.png, /images/relu.png 1.5x, /images/relu.png 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
得到输出reluOut
dropout层：
<img loading=lazy src=/images/dropout.png srcset="/images/dropout.png, /images/dropout.png 1.5x, /images/dropout.png 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
得到输出dropOut
   然后在第L层也就是最后一层先进入一个affine层，然后把结果进入一个softmax层得到各个类别的分类概率。
在对每个样本softmax进行求loss得到最后的softmax loss，在加上正则化后为
$$L=\frac{1}{N}\sum_{i=1}^{N}Li(W) + \lambda \cdot \sum_{l} \sum_{i}\sum_{j}W_{ij}^{l}$$</p><div class=highlight id=id-1><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#代码解释</span>
</span></span><span class=line><span class=cl><span class=n>affOut</span><span class=p>,</span><span class=n>affCache</span> <span class=o>=</span> <span class=n>affine_forward</span><span class=p>(</span><span class=n>inputX</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>batchOut</span><span class=p>,</span><span class=n>batchCache</span> <span class=o>=</span> <span class=n>batchnorm_forward</span><span class=p>(</span><span class=n>affOut</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>beta</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>bn_params</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>reluOut</span><span class=p>,</span><span class=n>reluCache</span> <span class=o>=</span> <span class=n>relu_forward</span><span class=p>(</span><span class=n>batchOut</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dropOut</span><span class=p>,</span><span class=n>dropCache</span> <span class=o>=</span> <span class=n>dropout_forward</span><span class=p>(</span><span class=n>reluOut</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout_param</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><h2 id=3反向传播>3.反向传播</h2><p>   反向传播其实就是链式求导的一个应用，求loss函数对最后一层输入的求导为：
$$dZL = \frac{dL}{dZ} = \frac{1}{N}\sum_{N}^{i=1}\frac{dLi}{dZi}$$
到dropOut层反向传播（该层输入 reluOut， 输出 dropOut（当为L-1层的时候dropOut=ZL））：
$$\frac{dL}{dreluOut} = \frac{dL}{ddropOut} \cdot \frac{ddropOut}{dreluOut}$$
到relu层反向传播（该层输入 dbatchOut 输出 reluOut）：
$$\frac{dL}{dbatchOut} = \frac{dL}{dreluOut} \cdot \frac{dreluOut}{dbatchOut}$$
到batchout层（该层输入 affOut， 输出 batchOut）：
$$\frac{dL}{daffOut} = \frac{dL}{dbatchOut} \cdot \frac{dbatchOut}{daffOut}$$
$$d\gamma = \frac{dL}{d\gamma} = \frac{dL}{dbatchOut} \cdot \frac{dbatchOut}{daffOut}$$
$$d\beta = \frac{dL}{d\beta} = \frac{dL}{dbatchOut} \cdot \frac{dbatchOut}{daffOut}$$
到affine层（该层输入 X， 输出 affOut）：
$$dX = \frac{dL}{dX} = \frac{dL}{daffOut} \cdot \frac{daffOut}{dX}$$
$$dW = \frac{dL}{dW} = \frac{dL}{daffOut} \cdot \frac{daffOut}{dW}$$
$$db = \frac{dL}{db} = \frac{dL}{daffOut} \cdot \frac{daffOut}{db}$$</p><h2 id=4权重初始化>4.权重初始化</h2><p><img loading=lazy src=/images/neuralnetwork.png srcset="/images/neuralnetwork.png, /images/neuralnetwork.png 1.5x, /images/neuralnetwork.png 2x" sizes=auto data-title=image data-alt=image style="background:url(/svg/loading.min.svg)no-repeat 50%" onload='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e);this.dataset.lazyloaded=""' onerror='this.title=this.dataset.title,this.alt=this.dataset.alt;for(const e of["style","data-title","data-alt","onerror","onload"])this.removeAttribute(e)'>
   继续拿这个神经网络来说明，在第一层和第二层中我们都需要对权重进行初始化，每一层的w的维度初始化为本层的输入个数和本层的神经元个数，例如上图中第一层w的维度为3x4的矩阵，b的维度为1x4，第二层权重w的维度为4x2，b的维度为1x2。batchnorm层中beta和gamma的维度都为1xD（D为该层神经元的个数）。
   一般w为从高斯分布中均值为0进行初始化，b初始化为0矩阵，beta初始化为0，gamma初始化为1</p><h2 id=5神经网络中的梯度下降>5.神经网络中的梯度下降</h2><p>对每一层：
$$\gamma = \gamma - \alpha \cdot d\gamma$$
$$\beta = \beta - \alpha \cdot d\beta$$
$$W = W - \alpha \cdot dW$$
$$b = b - \alpha \cdot db$$
   然后前向传播求出loss，当loss足够小或者迭代次数足够多的时候停止梯度下降，此时参数即为近似最优解</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2023-03-28 13:49:36">更新于 2023-03-28&nbsp;<a class=git-hash href=https://github.com/SivanLaai/blog/commit/ca8657c97ac02015b35a1fa6378790703405de3c rel="external nofollow noopener noreferrer" target=_blank title="commit by SivanLaai(lyhhap@163.com) ca8657c97ac02015b35a1fa6378790703405de3c: update passages"><i class="fa-solid fa-hashtag fa-fw" aria-hidden=true></i>ca8657c</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/posts/research/neural_network/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span><span><a href=https://github.com/SivanLaai/blog/edit/master/content/posts/research/neural_network.md title=编辑此页 target=_blank rel="external nofollow noopener noreferrer" class=link-to-edit>编辑此页</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://www.laais.cn/posts/research/neural_network/ data-title=全连接神经网络 data-hashtags=神经网络><i class="fa-brands fa-twitter fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://www.laais.cn/posts/research/neural_network/ data-hashtag=神经网络><i class="fa-brands fa-facebook-square fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Hacker News" data-sharer=hackernews data-url=https://www.laais.cn/posts/research/neural_network/ data-title=全连接神经网络><i class="fa-brands fa-hacker-news fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://www.laais.cn/posts/research/neural_network/ data-title=全连接神经网络><i data-svg-src=/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://www.laais.cn/posts/research/neural_network/ data-title=全连接神经网络><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fa-solid fa-tags fa-fw me-1" aria-hidden=true></i><a href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/ class=post-tag>神经网络</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/research/softmax/ class=post-nav-item rel=prev title="Softmax 梯度下降优化"><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Softmax 梯度下降优化</a>
<a href=/posts/research/bacth_norm/ class=post-nav-item rel=next title=批量标准化>批量标准化<i class="fa-solid fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=waline class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://waline.js.org/ rel="external nofollow noopener noreferrer">Waline</a>.</noscript></div></article></main><footer class=footer><div class=footer-container><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2023</span><span class=author itemprop=copyrightHolder>
<a href=https://www.laais.cn target=_blank rel="external nofollow noopener noreferrer">SivanLaai</a></span></div><div class="footer-line statistics"><span class=site-time title><i class="fa-solid fa-heartbeat fa-fw animate-icon" aria-hidden=true></i>&nbsp;<span class=run-times></span></span></div><div class="footer-line beian"><a href=https://beian.miit.gov.cn class="icp footer-divider">京ICP备2023014024号-1</a></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class=variant-numeric>0%</span></div><div class="fixed-button view-comments d-none" role=button aria-label=查看评论><i class="fa-solid fa-comment fa-fw" aria-hidden=true></i></div></div><a href=https://github.com/SivanLaai/blog title="在 GitHub 上查看源代码" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0;--bg-progress:#000;--bg-progress-dark:#fff></div><noscript><div class=noscript-warning>FixIt 主题在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=stylesheet href=/lib/waline/waline.css><link rel=stylesheet href=/lib/katex/katex.min.css><script src=/lib/waline/waline.js defer></script><script src=/lib/autocomplete/autocomplete.min.js defer></script><script src=/lib/fuse/fuse.min.js defer></script><script src=/lib/sharer/sharer.min.js async defer></script><script src=/lib/katex/katex.min.js defer></script><script src=/lib/katex/auto-render.min.js defer></script><script src=/lib/katex/copy-tex.min.js defer></script><script src=/lib/katex/mhchem.min.js defer></script><script src=/lib/pangu/pangu.min.js defer></script><script src=/lib/cell-watermark/watermark.min.js defer></script><script>window.config={autoBookmark:!0,code:{copyTitle:"复制到剪贴板",editLockTitle:"锁定可编辑代码块",editUnLockTitle:"解锁可编辑代码块",editable:!0,maxShownLines:10},comment:{enable:!0,expired:!1,waline:{copyright:!0,dark:"body[data-theme='dark']",el:"#waline",emoji:["//unpkg.com/@waline/emojis@1.1.0/weibo"],highlighter:!1,imageUploader:!1,lang:"zh-cn",login:"enable",meta:["nick","mail","link"],pageSize:10,pageview:!0,search:!1,serverURL:"https://comment.laais.cn/",texRenderer:!1}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:""},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1},watermark:{appendto:".wrapper>main",colspacing:30,content:'<img class="fixit-icon" src="/safari-pinned-tab.svg" alt="FixIt logo" /> FixIt 主题',enable:!0,fontfamily:"inherit",fontsize:.85,height:21,opacity:.0125,rotate:15,rowspacing:60,width:150}}</script><script src=/js/theme.min.js defer></script></body></html>